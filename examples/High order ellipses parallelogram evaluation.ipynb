{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Wavefront set extractor for high order ellipses/parallelogram evaluation </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dense.shearlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import dense.batchgen as bg\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data\n",
    "with h5py.File(\"angle2ellipsesparalel_highorder.h5\", 'r') as h5:\n",
    "    X_train = h5[\"X_train\"][:]\n",
    "    X_test = h5[\"X_test\"][:]\n",
    "    X_valid = h5[\"X_valid\"][:]\n",
    "    y_train = h5[\"y_train\"][:]\n",
    "    y_test = h5[\"y_test\"][:]\n",
    "    y_valid = h5[\"y_valid\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_valid = y_valid.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2990, 20, 20, 49),\n",
       " (935, 20, 20, 49),\n",
       " (747, 20, 20, 49),\n",
       " (2990,),\n",
       " (935,),\n",
       " (747,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, X_valid.shape, y_train.shape, y_test.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU 0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from adler.tensorflow import prelu, cosine_decay\n",
    "import os\n",
    "import adler\n",
    "adler.util.gpu.setup_one_gpu()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "#name = os.path.splitext(os.path.basename(__file__))[0]\n",
    "name = os.path.splitext(os.getcwd())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To categorical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes = 2)\n",
    "y_test = to_categorical(y_test, num_classes = 2)\n",
    "y_valid = to_categorical(y_valid, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = X_train.shape[1]\n",
    "height = X_train.shape[2]\n",
    "channels = X_train.shape[3]\n",
    "nLabel = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weight Initialization\n",
    "# Create lots of weights and biases & Initialize with a small positive number as we will use ReLU\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "## Convolution and Pooling\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') \n",
    "\n",
    "## Pooling: max pooling over 2x2 blocks\n",
    "def max_pool_2x2(x): \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = 4\n",
    "fully_connected = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_conv1 (?, 20, 20, 196)\n",
      "h_pool1 (?, 20, 20, 196)\n",
      "h_conv2 (?, 20, 20, 784)\n",
      "h_pool2 (?, 10, 10, 784)\n",
      "h_conv3 (?, 10, 10, 1568)\n",
      "h_pool3 (?, 5, 5, 1568)\n",
      "h_conv4 (?, 5, 5, 3136)\n",
      "h_pool4 (?, 3, 3, 3136)\n",
      "h_pool4_flat (?, 28224)\n",
      "WARNING:tensorflow:From <ipython-input-15-e3b4a9056eb3>:104: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.float32, shape=(None, width, height,channels))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, nLabel))\n",
    "    \n",
    "    ## First Convolutional Layer\n",
    "    W_conv1 = weight_variable([3, 3, 49, 49*4])\n",
    "    b_conv1 = bias_variable([49*4])\n",
    "    #Convolution\n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    print('h_conv1',h_conv1.shape)\n",
    "    \n",
    "    # Batch normalization\n",
    "    # Calculate batch mean and variance\n",
    "    batch_mean1, batch_var1 = tf.nn.moments(h_conv1,[0])\n",
    "    h_conv1hat = (h_conv1-batch_mean1) / tf.sqrt(batch_var1 + 1e-3)\n",
    "    # Pooling\n",
    "    #h_pool1 = max_pool_2x2(h_conv1hat) \n",
    "    #print('h_pool1',h_pool1.shape)\n",
    "    # No_pooling\n",
    "    h_pool1 = h_conv1hat\n",
    "    print('h_pool1',h_pool1.shape)\n",
    "    \n",
    "    ## Second Convolutional Layer\n",
    "    W_conv2 = weight_variable([3, 3, 49*4, 49*4*4])\n",
    "    b_conv2 = bias_variable([49*4*4])\n",
    "    #Convolution\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    print('h_conv2',h_conv2.shape)\n",
    "    \n",
    "    # Batch normalization\n",
    "    # Calculate batch mean and variance\n",
    "    batch_mean2, batch_var2 = tf.nn.moments(h_conv2,[0])\n",
    "    h_conv2hat = (h_conv2-batch_mean2) / tf.sqrt(batch_var2 + 1e-3)\n",
    "    \n",
    "    # Pooling\n",
    "    h_pool2 = max_pool_2x2(h_conv2hat) \n",
    "    print('h_pool2',h_pool2.shape)\n",
    "    \n",
    "    ## Third Convolutional Layer\n",
    "    W_conv3 = weight_variable([3, 3, 49*4*4, 49*4*4*2])\n",
    "    b_conv3 = bias_variable([49*4*4*2])\n",
    "    #Convolution\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "    print('h_conv3',h_conv3.shape)\n",
    "    \n",
    "    # Batch normalization\n",
    "    # Calculate batch mean and variance\n",
    "    batch_mean3, batch_var3 = tf.nn.moments(h_conv3,[0])\n",
    "    h_conv3hat = (h_conv3-batch_mean3) / tf.sqrt(batch_var3 + 1e-3)\n",
    "    \n",
    "    # Pooling\n",
    "    h_pool3 = max_pool_2x2(h_conv3hat) \n",
    "    print('h_pool3',h_pool3.shape)\n",
    "\n",
    "    \n",
    "    ## Third Convolutional Layer\n",
    "    W_conv4 = weight_variable([3, 3, 49*4*4*2, 49*4*4*2*2])\n",
    "    b_conv4 = bias_variable([49*4*4*2*2])\n",
    "    #Convolution\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "    print('h_conv4',h_conv4.shape)\n",
    "    \n",
    "    # Batch normalization\n",
    "    # Calculate batch mean and variance\n",
    "    batch_mean4, batch_var4 = tf.nn.moments(h_conv4,[0])\n",
    "    h_conv4hat = (h_conv4-batch_mean4) / tf.sqrt(batch_var4 + 1e-3)\n",
    "    \n",
    "    # Pooling\n",
    "    h_pool4 = max_pool_2x2(h_conv4hat) \n",
    "    print('h_pool4',h_pool4.shape)\n",
    "\n",
    "    ## Densely Connected Layer \n",
    "\n",
    "    # new shapes of pooled vectors\n",
    "    _, width_pooled, height_pooled, channels_pooled = h_pool4.shape\n",
    "\n",
    "    # fully-connected layer with 1024 neurons to process on the entire image\n",
    "    W_fc1 = weight_variable([int(width_pooled*height_pooled*channels_pooled), 1024])  \n",
    "    b_fc1 = bias_variable([1024])\n",
    "    \n",
    "    # Flat the output of the convolutional labels\n",
    "    h_pool4_flat = tf.reshape(h_pool4, [-1, int(width_pooled*height_pooled*channels_pooled)])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    print('h_pool4_flat',h_pool4_flat.shape)\n",
    "\n",
    "    ## Dropout (to reduce overfitting; useful when training very large neural network)\n",
    "    # We will turn on dropout during training & turn off during testing\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    ## Readout Layer\n",
    "\n",
    "    W_fc2 = weight_variable([1024, nLabel]) # [1024, 10]\n",
    "    b_fc2 = bias_variable([nLabel]) # [10]\n",
    "    \n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    predict = tf.nn.softmax(y_conv)\n",
    "\n",
    "    # set up for optimization (optimizer:ADAM)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)  # 1e-4\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "    mf_score = tf.contrib.metrics.f1_score(tf.argmax(y_conv,1),tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.contrib.metrics.python.metrics.classification.f1_score(labels, predictions, weights=None, num_thresholds=200, metrics_collections=None, updates_collections=None, name=None)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.contrib.metrics.f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy_minibatches = []\n",
    "batch_size_test = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_ellipsesparalel_highorder/model_angle2\n",
      "step 0, test accuracy 0.925\n",
      "step 5, test accuracy 0.941\n",
      "step 10, test accuracy 0.923\n",
      "step 15, test accuracy 0.941\n",
      "step 20, test accuracy 0.923\n",
      "step 25, test accuracy 0.941\n",
      "step 30, test accuracy 0.922\n",
      "step 35, test accuracy 0.943\n",
      "step 40, test accuracy 0.924\n",
      "step 45, test accuracy 0.942\n",
      "step 50, test accuracy 0.926\n",
      "step 55, test accuracy 0.943\n",
      "step 60, test accuracy 0.927\n",
      "step 65, test accuracy 0.941\n",
      "step 70, test accuracy 0.929\n",
      "step 75, test accuracy 0.941\n",
      "step 80, test accuracy 0.929\n",
      "step 85, test accuracy 0.938\n",
      "step 90, test accuracy 0.93\n",
      "step 95, test accuracy 0.937\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    save_dir = 'checkpoints_ellipsesparalel_highorder/'\n",
    "    save_path = os.path.join(save_dir, 'model_angle2')\n",
    "    saver = tf.train.Saver()  # Gets all variables in `graph`.\n",
    "    saver.restore(sess=session, save_path=save_path)\n",
    "    for step in range(num_steps):\n",
    "            offset_test = (step * batch_size_test) % (y_test.shape[0] - batch_size_test)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data_test = X_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            batch_labels_test = y_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            test_accuracy = accuracy.eval(feed_dict={x:batch_data_test, y_: batch_labels_test, keep_prob: 1.0})\n",
    "            test_accuracy_minibatches.append(test_accuracy)\n",
    "\n",
    "            if step%5 == 0:\n",
    "                print(\"step %d, test accuracy %g\"%(step, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Computing f-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_ellipsesparalel_highorder/model_angle2\n",
      "step 0, mf-score 0.923\n",
      "step 5, mf-score 0.941\n",
      "step 10, mf-score 0.922\n",
      "step 15, mf-score 0.943\n",
      "step 20, mf-score 0.924\n",
      "step 25, mf-score 0.942\n",
      "step 30, mf-score 0.926\n",
      "step 35, mf-score 0.943\n",
      "step 40, mf-score 0.927\n",
      "step 45, mf-score 0.941\n",
      "step 50, mf-score 0.929\n",
      "step 55, mf-score 0.941\n",
      "step 60, mf-score 0.929\n",
      "step 65, mf-score 0.938\n",
      "step 70, mf-score 0.93\n",
      "step 75, mf-score 0.937\n",
      "step 80, mf-score 0.933\n",
      "step 85, mf-score 0.936\n",
      "step 90, mf-score 0.934\n",
      "step 95, mf-score 0.936\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    save_dir = 'checkpoints_ellipsesparallel/'\n",
    "    save_path = os.path.join(save_dir, 'model_angle2')\n",
    "    saver = tf.train.Saver()  # Gets all variables in `graph`.\n",
    "    saver.restore(sess=session, save_path=save_path)\n",
    "    for step in range(num_steps):\n",
    "            offset_test = (step * batch_size_test+100) % (y_test.shape[0] - batch_size_test)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data_test = X_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            batch_labels_test = y_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            test_accuracy = mf_score.eval(feed_dict={x:batch_data_test, y_: batch_labels_test, keep_prob: 1.0})\n",
    "            test_accuracy_minibatches.append(test_accuracy)\n",
    "\n",
    "            if step%5 == 0:\n",
    "                print(\"step %d, mf-score %g\"%(step, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, test accuracy 0.491979\n",
      "step 5, test accuracy 0.6\n",
      "step 10, test accuracy 0.6\n",
      "step 15, test accuracy 0.6\n",
      "step 20, test accuracy 0.6\n",
      "step 25, test accuracy 0.48\n",
      "step 30, test accuracy 0.5\n",
      "step 35, test accuracy 0.485714\n",
      "step 40, test accuracy 0.5\n",
      "step 45, test accuracy 0.488889\n",
      "step 50, test accuracy 0.46\n",
      "step 55, test accuracy 0.454545\n",
      "step 60, test accuracy 0.483333\n",
      "step 65, test accuracy 0.491979\n",
      "step 70, test accuracy 0.6\n",
      "step 75, test accuracy 0.6\n",
      "step 80, test accuracy 0.6\n",
      "step 85, test accuracy 0.6\n",
      "step 90, test accuracy 0.48\n",
      "step 95, test accuracy 0.5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(num_steps):\n",
    "            offset_test = (step * batch_size_test) % (y_test.shape[0] - batch_size_test)\n",
    "\n",
    "            # Generate a minibatch.\n",
    "            batch_data_test = X_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            batch_labels_test = y_test[offset_test:(offset_test + batch_size_test), :]\n",
    "            test_accuracy = accuracy.eval(feed_dict={x:batch_data_test, y_: batch_labels_test, keep_prob: 1.0})\n",
    "            test_accuracy_minibatches.append(test_accuracy)\n",
    "\n",
    "            if step%5 == 0:\n",
    "                print(\"step %d, test accuracy %g\"%(step, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_ellipsesparalel_highorder/model_angle2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    save_dir = 'checkpoints_ellipsesparalel_highorder/'\n",
    "    save_path = os.path.join(save_dir, 'model_angle2')\n",
    "    saver = tf.train.Saver()  # Gets all variables in `graph`.\n",
    "    saver.restore(sess=session, save_path=save_path)\n",
    "    plt.imshow(W_conv1[:,:,0,3].eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = X_test[10:11,:,:,:]\n",
    "to_predict_label = y_test[10:11,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints_ellipsesparalel_highorder/model_angle2\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    save_dir = 'checkpoints_ellipsesparalel_highorder/'\n",
    "    save_path = os.path.join(save_dir, 'model_angle2')\n",
    "    saver = tf.train.Saver()  # Gets all variables in `graph`.\n",
    "    saver.restore(sess=session, save_path=save_path)\n",
    "    prediction2 = session.run(predict,feed_dict={x:to_predict,  keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4803189, 0.5196811]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
